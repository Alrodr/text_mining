{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA model with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Input data trigram_sentences_all.txt generated from yelp dataset using the notebook ....\n",
    "data_directory = '/Users/jorge/data/training/text'\n",
    "trigram_reviews_filepath = os.path.join(data_directory, 'trigram_sentences_all.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 48s, sys: 1.08 s, total: 2min 49s\n",
      "Wall time: 2min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "\n",
    "\n",
    "# Create dictionary\n",
    "trigram_dictionary_filepath = os.path.join(data_directory, 'trigram_dict_all.dict')\n",
    "\n",
    "trigram_reviews = LineSentence(trigram_reviews_filepath)\n",
    "\n",
    "# learn the dictionary by iterating over all of the reviews\n",
    "trigram_dictionary = Dictionary(trigram_reviews)\n",
    "    \n",
    "# filter tokens that are very rare or too common from\n",
    "# the dictionary (filter_extremes) and reassign integer ids (compactify)\n",
    "trigram_dictionary.filter_extremes(no_below=10, no_above=0.4)\n",
    "trigram_dictionary.compactify()\n",
    "\n",
    "trigram_dictionary.save(trigram_dictionary_filepath)\n",
    "\n",
    "# 2min 49s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create bag of words representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 53s, sys: 4.68 s, total: 3min 58s\n",
      "Wall time: 3min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "trigram_bow_filepath = os.path.join(data_directory, 'trigram_bow_corpus_all.mm')\n",
    "\n",
    "def trigram_bow_generator(filepath):\n",
    "    \"\"\"\n",
    "    generator function to read reviews from a file\n",
    "    and yield a bag-of-words representation\n",
    "    \"\"\"\n",
    "    for review in LineSentence(filepath):\n",
    "        yield trigram_dictionary.doc2bow(review)\n",
    "\n",
    "# generate bag-of-words representations for all reviews and save them as a matrix\n",
    "MmCorpus.serialize(trigram_bow_filepath, trigram_bow_generator(trigram_reviews_filepath))\n",
    "\n",
    "# load the finished bag-of-words corpus from disk\n",
    "trigram_bow_corpus = MmCorpus(trigram_bow_filepath)\n",
    "\n",
    "# 3min 58s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 4s, sys: 6min 42s, total: 16min 46s\n",
      "Wall time: 16min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import warnings\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "lda_model_filepath = os.path.join(data_directory, 'lda_model_all')\n",
    "if 1 == 1:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        \n",
    "        # workers => number of physical cores minus one\n",
    "        lda = LdaMulticore(trigram_bow_corpus,\n",
    "                           num_topics=50,\n",
    "                           id2word=trigram_dictionary,\n",
    "                           workers=7)\n",
    "    \n",
    "    lda.save(lda_model_filepath)\n",
    "# 16min 31s  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic topic exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_topic(topic_number, topn=25):\n",
    "    \"\"\"print out a formatted list of the top terms\n",
    "    \"\"\"\n",
    "    print('term - frequency\\n')\n",
    "    for term, frequency in lda.show_topic(topic_number, topn=topn):\n",
    "        print(term, round(frequency, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term - frequency\n",
      "\n",
      "very 0.219\n",
      "price 0.101\n",
      "and 0.074\n",
      "small 0.055\n",
      "portion 0.045\n",
      "for 0.042\n",
      "big 0.035\n",
      "large 0.031\n",
      "huge 0.029\n",
      "food 0.023\n",
      "enough 0.019\n",
      "reasonable 0.019\n",
      "but 0.019\n",
      "a 0.017\n",
      "size 0.016\n",
      "with 0.01\n",
      "seriously 0.01\n",
      "also 0.008\n",
      "yum 0.008\n",
      "greasy 0.008\n",
      "high 0.007\n",
      "fair 0.007\n",
      "japanese 0.006\n",
      "generous 0.006\n",
      "portion_size 0.006\n"
     ]
    }
   ],
   "source": [
    "explore_topic(topic_number=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced topic exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "LDAvis_data_filepath = os.path.join(data_directory, 'ldavis_prepared')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "LDAvis_prepared = pyLDAvis.gensim.prepare(lda, trigram_bow_corpus, trigram_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LDAvis_data_filepath, 'w') as f:\n",
    "    pickle.dump(LDAvis_prepared, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describing text with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_review(review_number):\n",
    "    \"\"\"\n",
    "    retrieve a particular review index\n",
    "    from the reviews file and return it\n",
    "    \"\"\"\n",
    "    \n",
    "    return list(it.islice(line_review(review_txt_filepath),\n",
    "                          review_number, review_number+1))[0]\n",
    "\n",
    "\n",
    "def lda_description(review_text, min_topic_freq=0.05):\n",
    "    \"\"\"\n",
    "    accept the original text of a review and (1) parse it with spaCy,\n",
    "    (2) apply text pre-proccessing steps, (3) create a bag-of-words\n",
    "    representation, (4) create an LDA representation, and\n",
    "    (5) print a sorted list of the top topics in the LDA representation\n",
    "    \"\"\"\n",
    "    \n",
    "    # parse the review text with spaCy\n",
    "    parsed_review = nlp(review_text)\n",
    "    \n",
    "    # lemmatize the text and remove punctuation and whitespace\n",
    "    unigram_review = [token.lemma_ for token in parsed_review\n",
    "                      if not punct_space(token)]\n",
    "    \n",
    "    # apply the first-order and secord-order phrase models\n",
    "    bigram_review = bigram_model[unigram_review]\n",
    "    trigram_review = trigram_model[bigram_review]\n",
    "    \n",
    "    # remove any remaining stopwords\n",
    "    trigram_review = [term for term in trigram_review\n",
    "                      if not term in spacy.en.STOPWORDS]\n",
    "    \n",
    "    # create a bag-of-words representation\n",
    "    review_bow = trigram_dictionary.doc2bow(trigram_review)\n",
    "    \n",
    "    # create an LDA representation\n",
    "    review_lda = lda[review_bow]\n",
    "    \n",
    "    # sort with the most highly related topics first\n",
    "    review_lda = sorted(review_lda, key=lambda (topic_number, freq): -freq)\n",
    "    \n",
    "    print()\n",
    "    for topic_number, freq in review_lda:\n",
    "        if freq < min_topic_freq:\n",
    "            break\n",
    "            \n",
    "        # print the most highly related topic names and frequencies\n",
    "        print(topic_names[topic_number], round(freq, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_review = get_sample_review(50)\n",
    "print(sample_review)\n",
    "\n",
    "lda_description(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_review = get_sample_review(100)\n",
    "print(sample_review)\n",
    "\n",
    "lda_description(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tm]",
   "language": "python",
   "name": "conda-env-tm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
