{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Yelp dataset \n",
    "- Select only comments of business=Restaurants whit size between 100 and 300 chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import json\n",
    "import itertools as it\n",
    "\n",
    "data_directory = '/home/ubuntu/data/training/text_mining/dataset'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"business_id\": \"FYWN1wneV18bWNgQjJ2GNg\", \"name\": \"Dental by Design\", \"neighborhood\": \"\", \"address\": \"4855 E Warner Rd, Ste B9\", \"city\": \"Ahwatukee\", \"state\": \"AZ\", \"postal_code\": \"85044\", \"latitude\": 33.3306902, \"longitude\": -111.9785992, \"stars\": 4.0, \"review_count\": 22, \"is_open\": 1, \"attributes\": {\"AcceptsInsurance\": true, \"ByAppointmentOnly\": true, \"BusinessAcceptsCreditCards\": true}, \"categories\": [\"Dentists\", \"General Dentistry\", \"Health & Medical\", \"Oral Surgeons\", \"Cosmetic Dentists\", \"Orthodontists\"], \"hours\": {\"Friday\": \"7:30-17:00\", \"Tuesday\": \"7:30-17:00\", \"Thursday\": \"7:30-17:00\", \"Wednesday\": \"7:30-17:00\", \"Monday\": \"7:30-17:00\"}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "businesses_filepath = os.path.join(data_directory, 'business.json')\n",
    "\n",
    "with codecs.open(businesses_filepath, encoding='utf_8') as f:\n",
    "    first_business_record = f.readline() \n",
    "\n",
    "print(first_business_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"review_id\":\"v0i_UHJMo_hPBq9bxWvW4w\",\"user_id\":\"bv2nCi5Qv5vroFiqKGopiw\",\"business_id\":\"0W4lkclzZThpx3V65bVgig\",\"stars\":5,\"date\":\"2016-05-28\",\"text\":\"Love the staff, love the meat, love the place. Prepare for a long line around lunch or dinner hours. \\n\\nThey ask you how you want you meat, lean or something maybe, I can't remember. Just say you don't want it too fatty. \\n\\nGet a half sour pickle and a hot pepper. Hand cut french fries too.\",\"useful\":0,\"funny\":0,\"cool\":0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_json_filepath = os.path.join(data_directory, 'review.json')\n",
    "\n",
    "with codecs.open(review_json_filepath, encoding='utf_8') as f:\n",
    "    first_review_record = f.readline()\n",
    "    \n",
    "print(first_review_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54618 restaurants in the dataset.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Select the business ids of the restaurants\n",
    "restaurant_ids = set()\n",
    "with codecs.open(businesses_filepath, encoding='utf_8') as f:\n",
    "    for business_json in f:\n",
    "        business = json.loads(business_json)\n",
    "        if u'Restaurants' not in business[u'categories']:\n",
    "            continue\n",
    "        restaurant_ids.add(business[u'business_id'])\n",
    "\n",
    "print(len(restaurant_ids), 'restaurants in the dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text from 974,066 restaurant reviews written to the new txt file.\n",
      "CPU times: user 2min 47s, sys: 2.32 s, total: 2min 49s\n",
      "Wall time: 2min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "review_count = 0\n",
    "\n",
    "# create & open a new file in write mode\n",
    "review_txt_filepath = os.path.join(data_directory, 'review_text_all.txt')\n",
    "\n",
    "with codecs.open(review_txt_filepath, 'w', encoding='utf_8') as review_txt_file:\n",
    "\n",
    "    # open the existing review json file\n",
    "    with codecs.open(review_json_filepath, encoding='utf_8') as review_json_file:\n",
    "\n",
    "        # loop through all reviews in the existing file and convert to dict\n",
    "        for review_json in review_json_file:\n",
    "            review = json.loads(review_json)\n",
    "\n",
    "            # if this review is not about a restaurant, skip to the next one\n",
    "            if review[u'business_id'] not in restaurant_ids:\n",
    "                continue\n",
    "\n",
    "            if len(review[u'text'])>600 or len(review[u'text'])<300:\n",
    "                continue\n",
    "                \n",
    "            # write the restaurant review as a line in the new file\n",
    "            # escape newline characters in the original review text\n",
    "            review_txt_file.write(review[u'text'].replace('\\n', '\\\\n') + '\\n')\n",
    "            review_count += 1\n",
    "\n",
    "print( u'''Text from {:,} restaurant reviews written to the new txt file.'''.format(review_count))\n",
    "# 1024739    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy NLP process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuation or whitespace\n",
    "    \"\"\"\n",
    "    \n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "def line_review(filename):\n",
    "    \"\"\"\n",
    "    generator function to read in reviews from the file\n",
    "    and un-escape the original line breaks in the text\n",
    "    \"\"\"\n",
    "    \n",
    "    with codecs.open(filename, encoding='utf_8') as f:\n",
    "        for review in f:\n",
    "            yield review.replace('\\\\n', '\\n')\n",
    "            \n",
    "def lemmatized_sentence_corpus(filename):\n",
    "    \"\"\"\n",
    "    generator function to use spaCy to parse reviews,\n",
    "    lemmatize the text, and yield sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    for parsed_review in nlp.pipe(line_review(filename),\n",
    "                                  batch_size=10000, n_threads=4):\n",
    "        \n",
    "        for sent in parsed_review.sents:\n",
    "            yield u' '.join([token.lemma_ for token in sent\n",
    "                             if not punct_space(token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# segmenting the reviews into individual sentences and normalizing the text\n",
    "unigram_sentences_filepath = os.path.join(data_directory, 'unigram_sentences_all.txt')\n",
    "with codecs.open(unigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "    for sentence in lemmatized_sentence_corpus(review_txt_filepath):\n",
    "        f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-PRON- advice be avoid this place if -PRON- can \n",
      "\n",
      "unfortunately -PRON- will probably eat here again as -PRON- be right across the street from -PRON- \n",
      "\n",
      "and there be few other choice in the wee hour of the morning \n",
      "\n",
      "when -PRON- be look for a nice quiet drink in a clean bar this be always the place -PRON- choose \n",
      "\n",
      "this be a place that -PRON- would refer to as a bar for adult as oppose to those place where all those kid hang out \n",
      "\n",
      "chance be -PRON- be not go to find a bunch of kid act like -PRON- be have there first beer \n",
      "\n",
      "if someone decide to have a shot -PRON- will not have to listen to all there friend yell shot shot shot shot good food clean good service \n",
      "\n",
      "a class place \n",
      "\n",
      "-PRON- have eat here several time as -PRON- work in the area \n",
      "\n",
      "as usual for a food court area in a casino -PRON- be always very loud here \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the content\n",
    "\n",
    "# LineSentence: iterator over a file with one sentence by line.\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "unigram_sentences = LineSentence(unigram_sentences_filepath)\n",
    "for unigram_sentence in it.islice(unigram_sentences, 230, 240):\n",
    "    print(u' '.join(unigram_sentence), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 13s, sys: 10.4 s, total: 3min 23s\n",
      "Wall time: 3min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Learn a phrase model that will link individual words into two-word phrases\n",
    "from gensim.models import Phrases\n",
    "\n",
    "bigram_model_filepath = os.path.join(data_directory, 'bigram_model_all')\n",
    "\n",
    "bigram_model = Phrases(unigram_sentences)\n",
    "bigram_model.save(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/anaconda3/envs/tm/lib/python3.5/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 3s, sys: 29.7 s, total: 9min 32s\n",
      "Wall time: 9min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# With the trained phrase model for word pairs, let's apply it to the review sentences data \n",
    "bigram_sentences_filepath = os.path.join(data_directory, 'bigram_sentences_all.txt')\n",
    "\n",
    "with codecs.open(bigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "    for unigram_sentence in unigram_sentences:\n",
    "        bigram_sentence = u' '.join(bigram_model[unigram_sentence])\n",
    "        f.write(bigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-PRON- advice be avoid this place if -PRON- can \n",
      "\n",
      "unfortunately -PRON- will probably eat here again as -PRON- be right across the street from -PRON- \n",
      "\n",
      "and there be few other choice in the wee hour of the morning \n",
      "\n",
      "when -PRON- be look for a nice quiet drink in a clean bar this be always the place -PRON- choose \n",
      "\n",
      "this be a place that -PRON- would refer to as a bar for adult as oppose to those place where all those kid hang out \n",
      "\n",
      "chance be -PRON- be not go to find a bunch of kid act like -PRON- be have there first beer \n",
      "\n",
      "if someone decide to have a shot -PRON- will not have to listen to all there friend yell shot shot shot shot good food clean good service \n",
      "\n",
      "a class place \n",
      "\n",
      "-PRON- have eat here several time as -PRON- work in the area \n",
      "\n",
      "as usual for a food court area in a casino -PRON- be always very loud here \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore the results\n",
    "bigram_sentences = LineSentence(bigram_sentences_filepath)\n",
    "for bigram_sentence in it.islice(bigram_sentences, 230, 240):\n",
    "    print(u' '.join(bigram_sentence), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 11s, sys: 11.7 s, total: 3min 22s\n",
      "Wall time: 3min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Learn a new phrase model over the previous results to obtain trigrams.\n",
    "trigram_model_filepath = os.path.join(data_directory, 'trigram_model_all')\n",
    "\n",
    "trigram_model = Phrases(bigram_sentences)\n",
    "trigram_model.save(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/anaconda3/envs/tm/lib/python3.5/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 59s, sys: 29.2 s, total: 9min 28s\n",
      "Wall time: 9min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Apply over the corpus\n",
    "trigram_sentences_filepath = os.path.join(data_directory, 'trigram_sentences_all.txt')\n",
    "\n",
    "with codecs.open(trigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "    for bigram_sentence in bigram_sentences:\n",
    "        trigram_sentence = u' '.join(trigram_model[bigram_sentence])\n",
    "        f.write(trigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-PRON- advice be avoid this place if -PRON- can \n",
      "\n",
      "unfortunately -PRON- will probably eat here again as -PRON- be right across the street from -PRON- \n",
      "\n",
      "and there be few other choice in the wee_hour of the morning \n",
      "\n",
      "when -PRON- be look for a nice quiet drink in a clean bar this be always the place -PRON- choose \n",
      "\n",
      "this be a place that -PRON- would refer to as a bar for adult as_oppose to those place where all those kid hang_out \n",
      "\n",
      "chance be -PRON- be not go to find a bunch of kid act like -PRON- be have there first beer \n",
      "\n",
      "if someone decide to have a shot -PRON- will not have to listen to all there friend yell shot shot shot shot good food clean good service \n",
      "\n",
      "a class place \n",
      "\n",
      "-PRON- have eat here several time as -PRON- work in the area \n",
      "\n",
      "as usual for a food court area in a casino -PRON- be always very loud here \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore the results\n",
    "trigram_sentences = LineSentence(trigram_sentences_filepath)\n",
    "for trigram_sentence in it.islice(trigram_sentences, 230, 240):\n",
    "    print(u' '.join(trigram_sentence), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "\n",
      "Small unassuming place that changes their menu every so often. Cool decor and vibe inside their 30 seat restaurant. Call for a reservation. \n",
      "\n",
      "We had their beef tartar and pork belly to start and a salmon dish and lamb meal for mains. Everything was incredible! I could go on at length about how all the listed ingredients really make their dishes amazing but honestly you just need to go. \n",
      "\n",
      "A bit outside of downtown montreal but take the metro out and it's less than a 10 minute walk from the station.\n",
      "\n",
      "Lester's is located in a beautiful neighborhood and has been there since 1951. They are known for smoked meat which most deli's have but their brisket sandwich is what I come to montreal for. They've got about 12 seats outside to go along with the inside. \n",
      "\n",
      "The smoked meat is up there in quality and taste with Schwartz's and you'll find less tourists at Lester's as well.\n",
      "\n",
      "Love coming here. Yes the place always needs the floor swept but when you give out  peanuts in the shell how won't it always be a bit dirty. \n",
      "\n",
      "The food speaks for itself, so good. Burgers are made to order and the meat is put on the grill when you order your sandwich. Getting the small burger just means 1 patty, the regular is a 2 patty burger which is twice the deliciousness. \n",
      "\n",
      "Getting the Cajun fries adds a bit of spice to them and whatever size you order they always throw more fries (a lot more fries) into the bag.\n",
      "\n",
      "----\n",
      "\n",
      "Transformed:\n",
      "\n",
      "small unassum place that change -PRON- menu every so often\n",
      "\n",
      "cool decor and vibe inside -PRON- 30 seat restaurant\n",
      "\n",
      "call for a reservation\n",
      "\n",
      "-PRON- have -PRON- beef_tartar and pork_belly to start and a salmon dish and lamb meal for main\n",
      "\n",
      "everything be incredible\n",
      "\n",
      "-PRON- could go on at length about how all the list ingredient really make -PRON- dish amazing but honestly -PRON- just need to go\n",
      "\n",
      "a bit outside of downtown montreal but take the metro out\n",
      "\n",
      "and -PRON- be less_than a 10_minute walk from the station\n",
      "\n",
      "lester_'s be locate in a beautiful neighborhood and have be there since_1951\n",
      "\n",
      "-PRON- be know for smoke_meat which most deli be have but -PRON- brisket sandwich be what -PRON- come to montreal for\n",
      "\n",
      "-PRON- have get about 12 seat outside to go along with the inside\n",
      "\n",
      "the smoked_meat be up there in quality and taste with schwartz_'s and -PRON- will find less tourist at lester_'s as well\n",
      "\n",
      "love come here\n",
      "\n",
      "yes the place always need the floor sweep but when -PRON- give out peanut in the shell how will not -PRON- always be a bit dirty\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check results of transformation\n",
    "print(u'Original:' + u'\\n')\n",
    "\n",
    "for review in it.islice(line_review(review_txt_filepath), 0, 3):\n",
    "    print(review)\n",
    "\n",
    "print(u'----' + u'\\n')\n",
    "print(u'Transformed:' + u'\\n')\n",
    "\n",
    "with codecs.open(trigram_sentences_filepath, encoding='utf_8') as f:\n",
    "    for review in it.islice(f, 0, 14):\n",
    "        print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete pipeline for new texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/anaconda3/envs/tm/lib/python3.5/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'spacy' has no attribute 'en'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'spacy' has no attribute 'en'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Final step:\n",
    "# - pipeline that applies our text normalization and phrase models\n",
    "# - remove stopwords\n",
    "# - write the transformed text out to a new file\n",
    "def apply_transformation(review_txt_filepath_in, trigram_reviews_filepath_out):\n",
    "\n",
    "    with codecs.open(trigram_reviews_filepath_out, 'w', encoding='utf_8') as f:\n",
    "\n",
    "        for parsed_review in nlp.pipe(line_review(review_txt_filepath_in), batch_size=10000, n_threads=4):\n",
    "\n",
    "            # lemmatize the text, removing punctuation and whitespace\n",
    "            unigram_review = [token.lemma_ for token in parsed_review\n",
    "                                  if not punct_space(token)]\n",
    "\n",
    "            # apply the first-order and second-order phrase models\n",
    "            bigram_review = bigram_model[unigram_review]\n",
    "            trigram_review = trigram_model[bigram_review]\n",
    "\n",
    "            # remove any remaining stopwords\n",
    "            trigram_review = [term for term in trigram_review if term not in spacy.en.STOPWORDS]\n",
    "\n",
    "            # write the transformed review as a line in the new file\n",
    "            trigram_review = u' '.join(trigram_review)\n",
    "            f.write(trigram_review + '\\n')\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tm]",
   "language": "python",
   "name": "conda-env-tm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
