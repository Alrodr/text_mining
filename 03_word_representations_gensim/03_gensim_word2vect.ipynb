{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with embedings using Gensim\n",
    "- Train word2vect embedings from a corpus\n",
    "- Load pretrained embedings\n",
    "- Use embedings to Classify text\n",
    "- Accest to the embedings of spaCy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Header\n",
    "import os\n",
    "\n",
    "import gensim, logging\n",
    "print('Gensim Version: ', gensim.__version__)\n",
    "\n",
    "\n",
    "# Access to the pretrained embedings\n",
    "data_path = '../data'\n",
    "\n",
    "# Access to the evaluation embedings file\n",
    "module_path = '/Users/jorge/anaconda3/envs/tm/lib/python3.6/site-packages/gensim/test'\n",
    "\n",
    "#'.../anaconda3/envs/tm/lib/site-packages/gensim/test''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code to build a word2vect embedings from a corpus\n",
    "\n",
    "# To show in the output the internal messages of the word2vect process\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    " \n",
    "# My little corpus    \n",
    "sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "\n",
    "# train word2vec on the two sentences\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a more big corpus\n",
    "from nltk.corpus import brown\n",
    "\n",
    "print('Corpus sentences len:', len(brown.sents()))\n",
    "print('Corpus words len:', len(brown.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a word2 vect model over the new corpus\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(brown.sents(), size=100, window=5, min_count=5, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Persist the model\n",
    "\n",
    "model.save('/tmp/brown_word2vect_model.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a trained model\n",
    "\n",
    "model = Word2Vec.load('/tmp/brown_word2vect_model.bin')  # you can continue training with the loaded model!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access to the embedings\n",
    "\n",
    "model.wv['the']  # Vector embeding of a word. Numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Similarity fucntions\n",
    "\n",
    "print('Similars to woman:', model.wv.most_similar_cosmul(positive=['woman']), '\\n')\n",
    "\n",
    "print(\"Indetify the word that doesn't match in a list:\", model.wv.doesnt_match(\"breakfast cereal dinner lunch\".split()), '\\n')\n",
    "\n",
    "print('Words similarity (woman - man):', model.wv.similarity('woman', 'man'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the accuracy of the builded embedings over a standar evaluation list of relations\n",
    "\n",
    "model.wv.accuracy(os.path.join(module_path, 'test_data', 'questions-words.txt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('BERLIN GERMANY PARIS FRANCE = ', model.wv.most_similar(positive=['germany', 'paris'], negative=['berlin']), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you finish to train the model. Save only the embedings and delete model.\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word_vectors = model.wv\n",
    "\n",
    "fname = '../data/word_vectors.gz'\n",
    "word_vectors.save(fname)\n",
    "\n",
    "del model\n",
    "\n",
    "# To load:\n",
    "# word_vectors = KeyedVectors.load(fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the embedings\n",
    "\n",
    "print('Num of embedings:', len(word_vectors.vocab.keys()), '\\n')\n",
    "\n",
    "print('Sample of words available (20 first):', list(word_vectors.vocab.keys())[:20], '\\n')\n",
    "\n",
    "print('Vocab word attributes for \"Oregon\" word:', word_vectors.vocab['Oregon'], '\\n')\n",
    "\n",
    "print('Word embedings for \"Oregon\" word:', word_vectors['Oregon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Vocabulary frequency. List the words with freq > 1000\n",
    "\n",
    "for k in word_vectors.vocab.keys():\n",
    "    if word_vectors.vocab[k].count > 1000:\n",
    "        print(k, word_vectors.vocab[k].count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained vectors and use it\n",
    "\n",
    "Load pretrained vectors (300Mb) from\n",
    " http://nlpserver2.inf.ufrgs.br/alexandres/vectors/lexvec.enwiki%2bnewscrawl.300d.W.pos.vectors.gz \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained embedings\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(os.path.join(data_path, 'lexvec.enwiki+newscrawl.300d.W.pos.vectors'),\n",
    "                                          unicode_errors='ignore')\n",
    "\n",
    "print('Sample of one embeding')\n",
    "dog = model['dog']\n",
    "print('Shape of one embeding:', dog.shape)\n",
    "print('First 10 embedings of \"dog\":', dog[:10], '\\n')\n",
    "\n",
    "\n",
    "# Some predefined functions that show content related information for given words\n",
    "print('woman + king - man = ', model.most_similar(positive=['woman', 'king'], negative=['man']), '\\n')\n",
    "\n",
    "print(\"Doesn't match:\", model.doesnt_match(\"breakfast cereal dinner lunch\".split()), '\\n')\n",
    "\n",
    "print('Similarity woman - man:', model.similarity('woman', 'man'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the accuracy of this model\n",
    "model.wv.accuracy(os.path.join(module_path, 'test_data', 'questions-words.txt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use in a model\n",
    "import numpy as np\n",
    "\n",
    "# Load data. Sentiment model in movies reviews.\n",
    "# Reference: http://www.aclweb.org/anthology/P11-1015 \n",
    "\n",
    "X_trn = np.load(os.path.join(data_path, 'sentiment_corpus', 'sentiment_X_trn.npy')) \n",
    "X_tst = np.load(os.path.join(data_path, 'sentiment_corpus', 'sentiment_X_tst.npy'))\n",
    "y_trn = np.load(os.path.join(data_path, 'sentiment_corpus', 'sentiment_y_trn.npy')) # 1: pos, 0:neg\n",
    "y_tst = np.load(os.path.join(data_path, 'sentiment_corpus', 'sentiment_y_tst.npy')) # 1: pos, 0:neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_trn.shape)\n",
    "print(X_trn[:2])\n",
    "print(y_trn[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represent each sentence by the average embeding of the words located in the embedings dictionary\n",
    "\n",
    "def encode_text(corpus, model):\n",
    "    '''\n",
    "    Function to encode text sentences into one embbeding by sentence\n",
    "        input: A list of sentences (corpus) and a embeddings model (model)\n",
    "        output: One embeding for each sentence (average of embedings of the words in the sentence)\n",
    "    '''\n",
    "    features_list = []\n",
    "    for s in corpus:\n",
    "        features = []\n",
    "        for t in s:\n",
    "            if str.lower(t) in model.vocab.keys():\n",
    "                features += [model[str.lower(t)]]\n",
    "        features_list += [np.mean(features, axis=0)] \n",
    "    return np.array(features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check embedings shape\n",
    "embeds_trn = encode_text(X_trn, model)\n",
    "print('Embeds trn shape:', embeds_trn.shape)\n",
    "\n",
    "embeds_tst = encode_text(X_tst, model)\n",
    "print('Embeds tst shape:', embeds_tst.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model and evaluate it\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Train\n",
    "text_clf_svm = LinearSVC()\n",
    "text_clf_svm.fit(embeds_trn, y_trn)\n",
    "\n",
    "#Evaluate test data\n",
    "predicted = text_clf_svm.predict(embeds_tst)\n",
    "print('Test accuracy:', np.mean(predicted == y_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedings in spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load a model with embedings . The small models don't have embedings\n",
    "import en_core_web_md\n",
    "nlp = en_core_web_sm.load()\n",
    "print('Model loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chech if the token has a vector.\n",
    "tokens = nlp(u'dog cat banana bibliopole')\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a given document, calculate similarity between 'apples' and 'oranges' and 'boots' and 'hippos'\n",
    "doc = nlp(u\"Apples and oranges are similar. Boots and hippos aren't.\")\n",
    "apples = doc[0]\n",
    "oranges = doc[2]\n",
    "boots = doc[6]\n",
    "hippos = doc[8]\n",
    "print('Similarity between words:')\n",
    "print('apples vs oranges: ', apples.similarity(oranges))\n",
    "print('boots vs hippos:', boots.similarity(hippos))\n",
    "\n",
    "print()\n",
    "print('Similarity between a word and a sentence:')\n",
    "# Print similarity between sentence and word 'fruit'\n",
    "apples_sent, boots_sent = doc.sents\n",
    "fruit = doc.vocab[u'fruit']\n",
    "print('apples sentence vs fruit word: ', apples_sent.similarity(fruit))\n",
    "print('boots sentence vs fruit word:', boots_sent.similarity(fruit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show vectors\n",
    "apples.vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tm]",
   "language": "python",
   "name": "conda-env-tm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
