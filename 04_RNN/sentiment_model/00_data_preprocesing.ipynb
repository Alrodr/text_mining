{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "    - Download data to the server\n",
    "    - Convert text to sequences.\n",
    "    - Configure sequences for a RNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data to the server\n",
    "\n",
    "### Command line in the server\n",
    "    Path to data:\n",
    "        cd /home/ubuntu/data/training/text/sentiment\n",
    "    Download dataset: \n",
    "        wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "    Uncompress it:\n",
    "        tar -zxvf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert text to sequences\n",
    "    - List of all text files\n",
    "    - Read files into python\n",
    "    - Tokenize\n",
    "    - Create dictionaries to recode\n",
    "    - Recode tokens into ids and create sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports and paths\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# GPU path\n",
    "#data_path='/home/ubuntu/data/training/text/sentiment/aclImdb/'\n",
    "\n",
    "data_path='../../data/aclImdb/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator of list of files in a folder and subfolders\n",
    "import os\n",
    "import shutil\n",
    "import fnmatch\n",
    "\n",
    "def gen_find(filepattern, toppath):\n",
    "    '''\n",
    "    Generator with a recursive list of files in the toppath that match filepattern \n",
    "    Inputs:\n",
    "        filepattern(str): Command stype pattern \n",
    "        toppath(str): Root path\n",
    "    '''\n",
    "    for path, dirlist, filelist in os.walk(toppath):\n",
    "        for name in fnmatch.filter(filelist, filepattern):\n",
    "            yield os.path.join(path, name)\n",
    "\n",
    "#Test\n",
    "print(next(gen_find(\"*.txt\", data_path+'train/pos/')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni character is an absolute scream. Watch for Alan \"The Skipper\" Hale jr. as a police Sgt.', 'Bizarre horror movie filled with famous faces but stolen by Cristina Raines (later of TV\\'s \"Flamingo Road\") as a pretty but somewhat unstable model with a gummy smile who is slated to pay for her attempted suicides by guarding the Gateway to Hell! The scenes with Raines modeling are very well captured, the mood music is perfect, Deborah Raffin is charming as Cristina\\'s pal, but when Raines moves into a creepy Brooklyn Heights brownstone (inhabited by a blind priest on the top floor), things really start cooking. The neighbors, including a fantastically wicked Burgess Meredith and kinky couple Sylvia Miles & Beverly D\\'Angelo, are a diabolical lot, and Eli Wallach is great fun as a wily police detective. The movie is nearly a cross-pollination of \"Rosemary\\'s Baby\" and \"The Exorcist\"--but what a combination! Based on the best-seller by Jeffrey Konvitz, \"The Sentinel\" is entertainingly spooky, full of shocks brought off well by director Michael Winner, who mounts a thoughtfully downbeat ending with skill. ***1/2 from ****']\n"
     ]
    }
   ],
   "source": [
    "def read_sentences(path):\n",
    "    sentences = []\n",
    "    sentences_list = gen_find(\"*.txt\", path)\n",
    "    for ff in sentences_list:\n",
    "        with open(ff, 'r', encoding='utf8') as f:\n",
    "            sentences.append(f.readline().strip())\n",
    "    return sentences        \n",
    "\n",
    "#Test\n",
    "print(read_sentences(data_path+'train/pos/')[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Working with one of the best Shakespeare sources, this film manages to be creditable to it's source, whilst still appealing to a wider audience.<br /><br />Branagh steals the film from under Fishburne's nose, and there's a talented cast on good form.\", 'Well...tremors I, the original started off in 1990 and i found the movie quite enjoyable to watch. however, they proceeded to make tremors II and III. Trust me, those movies started going downhill right after they finished the first one, i mean, ass blasters??? Now, only God himself is capable of answering the question \"why in Gods name would they create another one of these dumpster dives of a movie?\" Tremors IV cannot be considered a bad movie, in fact it cannot be even considered an epitome of a bad movie, for it lives up to more than that. As i attempted to sit though it, i noticed that my eyes started to bleed, and i hoped profusely that the little girl from the ring would crawl through the TV and kill me. did they really think that dressing the people who had stared in the other movies up as though they we\\'re from the wild west would make the movie (with the exact same occurrences) any better? honestly, i would never suggest buying this movie, i mean, there are cheaper ways to find things that burn well.']\n"
     ]
    }
   ],
   "source": [
    "print(read_sentences(data_path+'train/neg/')[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "Done!\n",
      "[['For', 'a', 'movie', 'that', 'gets', 'no', 'respect', 'there', 'sure', 'are', 'a', 'lot', 'of', 'memorable', 'quotes', 'listed', 'for', 'this', 'gem', '.', 'Imagine', 'a', 'movie', 'where', 'Joe', 'Piscopo', 'is', 'actually', 'funny', '!', 'Maureen', 'Stapleton', 'is', 'a', 'scene', 'stealer', '.', 'The', 'Moroni', 'character', 'is', 'an', 'absolute', 'scream', '.', 'Watch', 'for', 'Alan', '``', 'The', 'Skipper', \"''\", 'Hale', 'jr.', 'as', 'a', 'police', 'Sgt', '.'], ['Bizarre', 'horror', 'movie', 'filled', 'with', 'famous', 'faces', 'but', 'stolen', 'by', 'Cristina', 'Raines', '(', 'later', 'of', 'TV', \"'s\", '``', 'Flamingo', 'Road', \"''\", ')', 'as', 'a', 'pretty', 'but', 'somewhat', 'unstable', 'model', 'with', 'a', 'gummy', 'smile', 'who', 'is', 'slated', 'to', 'pay', 'for', 'her', 'attempted', 'suicides', 'by', 'guarding', 'the', 'Gateway', 'to', 'Hell', '!', 'The', 'scenes', 'with', 'Raines', 'modeling', 'are', 'very', 'well', 'captured', ',', 'the', 'mood', 'music', 'is', 'perfect', ',', 'Deborah', 'Raffin', 'is', 'charming', 'as', 'Cristina', \"'s\", 'pal', ',', 'but', 'when', 'Raines', 'moves', 'into', 'a', 'creepy', 'Brooklyn', 'Heights', 'brownstone', '(', 'inhabited', 'by', 'a', 'blind', 'priest', 'on', 'the', 'top', 'floor', ')', ',', 'things', 'really', 'start', 'cooking', '.', 'The', 'neighbors', ',', 'including', 'a', 'fantastically', 'wicked', 'Burgess', 'Meredith', 'and', 'kinky', 'couple', 'Sylvia', 'Miles', '&', 'Beverly', \"D'Angelo\", ',', 'are', 'a', 'diabolical', 'lot', ',', 'and', 'Eli', 'Wallach', 'is', 'great', 'fun', 'as', 'a', 'wily', 'police', 'detective', '.', 'The', 'movie', 'is', 'nearly', 'a', 'cross-pollination', 'of', '``', 'Rosemary', \"'s\", 'Baby', \"''\", 'and', '``', 'The', 'Exorcist', \"''\", '--', 'but', 'what', 'a', 'combination', '!', 'Based', 'on', 'the', 'best-seller', 'by', 'Jeffrey', 'Konvitz', ',', '``', 'The', 'Sentinel', \"''\", 'is', 'entertainingly', 'spooky', ',', 'full', 'of', 'shocks', 'brought', 'off', 'well', 'by', 'director', 'Michael', 'Winner', ',', 'who', 'mounts', 'a', 'thoughtfully', 'downbeat', 'ending', 'with', 'skill', '.', '***1/2', 'from', '****']]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(sentences):\n",
    "    from nltk import word_tokenize\n",
    "    print( 'Tokenizing...',)\n",
    "    tokens = []\n",
    "    for sentence in sentences:\n",
    "        tokens += [word_tokenize(sentence)]\n",
    "    print('Done!')\n",
    "\n",
    "    return tokens\n",
    "\n",
    "print(tokenize(read_sentences(data_path+'train/pos/')[0:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "Done!\n",
      "Tokenizing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "sentences_trn_pos = tokenize(read_sentences(data_path+'train/pos/'))\n",
    "sentences_trn_neg = tokenize(read_sentences(data_path+'train/neg/'))\n",
    "sentences_trn = sentences_trn_pos + sentences_trn_neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary..\n",
      "7056532  total words  134957  unique words\n",
      "2 289300\n"
     ]
    }
   ],
   "source": [
    "#create the dictionary to conver words to numbers. Order it with most frequent words first\n",
    "def build_dict(sentences):\n",
    "#    from collections import OrderedDict\n",
    "\n",
    "    '''\n",
    "    Build dictionary of train words\n",
    "    Outputs: \n",
    "     - Dictionary of word --> word index\n",
    "     - Dictionary of word --> word count freq\n",
    "    '''\n",
    "    print( 'Building dictionary..',)\n",
    "    wordcount = dict()\n",
    "    #For each worn in each sentence, cummulate frequency\n",
    "    for ss in sentences:\n",
    "        for w in ss:\n",
    "            if w not in wordcount:\n",
    "                wordcount[w] = 1\n",
    "            else:\n",
    "                wordcount[w] += 1\n",
    "\n",
    "    counts = list(wordcount.values()) # List of frequencies\n",
    "    keys = list(wordcount) #List of words\n",
    "    \n",
    "    sorted_idx = reversed(np.argsort(counts))\n",
    "    \n",
    "    worddict = dict()\n",
    "    for idx, ss in enumerate(sorted_idx):\n",
    "        worddict[keys[ss]] = idx+2  # leave 0 and 1 (UNK)\n",
    "    print( np.sum(counts), ' total words ', len(keys), ' unique words')\n",
    "\n",
    "    return worddict, wordcount\n",
    "\n",
    "\n",
    "worddict, wordcount = build_dict(sentences_trn)\n",
    "\n",
    "print(worddict['the'], wordcount['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def generate_sequence(sentences, dictionary):\n",
    "    '''\n",
    "    Convert tokenized text in sequences of integers\n",
    "    '''\n",
    "    seqs = [None] * len(sentences)\n",
    "    for idx, ss in enumerate(sentences):\n",
    "        seqs[idx] = [dictionary[w] if w in dictionary else 1 for w in ss]\n",
    "\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[333, 6, 25, 17, 225, 85, 1202, 68, 300, 35, 6, 189, 7, 918, 4813, 3630, 24, 19, 1573, 4, 4274, 6, 25, 141, 902, 16036, 9, 183, 182, 41, 7659, 16758, 9, 6, 157, 21658, 4, 21, 37442, 123, 9, 46, 1632, 3211, 4, 1231, 24, 1541, 32, 21, 24735, 31, 9450, 47112, 22, 6, 679, 7545, 4] 1\n"
     ]
    }
   ],
   "source": [
    "# Create train and test data\n",
    "\n",
    "#Read train sentences and generate target y\n",
    "train_x_pos = generate_sequence(sentences_trn_pos, worddict)\n",
    "train_x_neg = generate_sequence(sentences_trn_neg, worddict)\n",
    "X_train_full = train_x_pos + train_x_neg\n",
    "y_train_full = [1] * len(train_x_pos) + [0] * len(train_x_neg)\n",
    "\n",
    "print(X_train_full[0], y_train_full[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "Done!\n",
      "Tokenizing...\n",
      "Done!\n",
      "[4136, 33, 46, 772, 80, 3, 320, 13470, 299, 2, 1655, 7, 46, 338, 1251, 3, 625, 631, 5, 531, 81, 2023, 5, 75, 20, 6280, 9213, 23, 52, 1910, 4, 137, 3778, 8, 57024, 23, 52, 853, 474, 50, 6, 63, 339, 8, 98, 271, 49, 16, 45, 3, 29, 73, 52, 35283, 20, 2649, 14, 1, 3, 75, 96, 36, 604, 2, 757, 23, 52, 853, 3, 5, 20, 911, 8, 864, 171, 377, 75, 96, 98, 81565, 4, 7782, 49, 2, 338, 33204, 4, 415, 2224, 14, 6, 317, 187, 75, 96, 2609, 58, 3, 75, 569, 6, 1233, 97, 2, 4428, 23, 6, 3707, 4909, 4, 32, 15, 802, 1640, 166, 14, 172, 4713, 15090, 3, 29, 194, 16988, 14, 87, 4, 15, 20, 4713, 559, 4, 31, 12, 13, 10, 11, 12, 13, 10, 11, 7103, 45, 780, 3184, 1969, 5, 75, 20, 1057, 14, 6, 1012, 11101, 4, 565, 73, 16, 613, 50, 75, 76, 4112, 5, 7954, 24619, 6, 1552, 3, 75, 234, 52, 3707, 4909, 98, 3857, 5, 351, 4, 142, 6, 3519, 380, 75, 858, 8, 1927, 49, 2, 781, 1552, 5, 373, 8, 2392, 104, 3, 23, 85, 215, 7, 769, 4, 97780, 52, 144, 20, 14, 2610, 4, 12, 13, 10, 11, 12, 13, 10, 11, 897, 9, 6, 272, 47, 7123, 9639, 3, 20009, 12681, 3, 20913, 52, 144, 8, 671, 198, 4, 5824, 7039, 9, 334, 3, 5, 36, 57, 8, 192, 43, 125, 75, 56, 6, 334, 581, 4, 61, 9, 46, 3414, 80, 4, 12, 13, 10, 11, 12, 13, 10, 11, 31, 335, 35, 4626, 17, 2302, 9, 2, 42, 2466, 17, 144, 228, 920, 4, 31]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#Read test sentences and generate target y\n",
    "sentences_tst_pos = read_sentences(data_path+'test/pos/')\n",
    "sentences_tst_neg = read_sentences(data_path+'test/neg/')\n",
    "\n",
    "test_x_pos = generate_sequence(tokenize(sentences_tst_pos), worddict)\n",
    "test_x_neg = generate_sequence(tokenize(sentences_tst_neg), worddict)\n",
    "X_test_full = test_x_pos + test_x_neg\n",
    "y_test_full = [1] * len(test_x_pos) + [0] * len(test_x_neg)\n",
    "\n",
    "print(X_test_full[0])\n",
    "print(y_test_full[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure sequences for a RNN model\n",
    "    - Remove words with low frequency\n",
    "    - Truncate / complete sequences to the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median length:  208.0\n"
     ]
    }
   ],
   "source": [
    "#Median length of sentences\n",
    "print('Median length: ', np.median([len(x) for x in X_test_full]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 50000 # Number of most frequent words selected. the less frequent recode to 0\n",
    "maxlen = 200  # cut texts after this number of words (among top max_features most common words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61, 9, 6, 1573, 4, 214, 6, 1353, 4934, 391, 91, 2, 7528, 529, 20, 1019, 2106, 4, 6659, 23, 106, 460, 17, 1578, 87, 62, 6409, 4628, 127, 3, 108, 7565, 5, 353, 3222, 4, 51, 18, 239, 294, 4809, 8, 2, 246, 16, 18, 7551, 4, 335, 139, 166, 105, 602, 28, 42, 71, 521, 44, 2, 593, 7, 10670, 7, 6, 421, 14, 2, 3096, 27, 29, 100, 35, 4956, 8, 84, 16, 3, 6, 244, 50, 6, 549, 1753, 14, 634, 1559, 4, 21, 80, 14492, 107, 18695, 1379, 5, 1374, 62, 3793, 89, 36, 373, 5, 13793, 49, 801, 2, 49224, 3085, 7, 2, 561, 3, 22, 111, 1993, 23, 2, 3517, 7, 2, 101, 1749, 337, 543, 104, 3, 1504, 188, 48, 89, 30, 672, 104, 14, 74, 4, 51, 569, 87, 6, 184, 646, 8, 98, 58, 7, 2, 8607, 17, 17517, 87, 138, 342, 19, 0, 109, 704, 4]\n"
     ]
    }
   ],
   "source": [
    "#Select the most frequent max_features, recode others using 0\n",
    "def remove_features(x):\n",
    "    return [[0 if w >= max_features else w for w in sen] for sen in x]\n",
    "\n",
    "X_train = remove_features(X_train_full)\n",
    "X_test  = remove_features(X_test_full)\n",
    "y_train = y_train_full\n",
    "y_test = y_test_full\n",
    "\n",
    "print(X_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jorge/anaconda3/envs/tm/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jorge/anaconda3/envs/tm/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (25000, 200)\n",
      "X_test shape: (25000, 200)\n",
      "[    2   338 33204     4   415  2224    14     6   317   187    75    96\n",
      "  2609    58     3    75   569     6  1233    97     2  4428    23     6\n",
      "  3707  4909     4    32    15   802  1640   166    14   172  4713 15090\n",
      "     3    29   194 16988    14    87     4    15    20  4713   559     4\n",
      "    31    12    13    10    11    12    13    10    11  7103    45   780\n",
      "  3184  1969     5    75    20  1057    14     6  1012 11101     4   565\n",
      "    73    16   613    50    75    76  4112     5  7954 24619     6  1552\n",
      "     3    75   234    52  3707  4909    98  3857     5   351     4   142\n",
      "     6  3519   380    75   858     8  1927    49     2   781  1552     5\n",
      "   373     8  2392   104     3    23    85   215     7   769     4     0\n",
      "    52   144    20    14  2610     4    12    13    10    11    12    13\n",
      "    10    11   897     9     6   272    47  7123  9639     3 20009 12681\n",
      "     3 20913    52   144     8   671   198     4  5824  7039     9   334\n",
      "     3     5    36    57     8   192    43   125    75    56     6   334\n",
      "   581     4    61     9    46  3414    80     4    12    13    10    11\n",
      "    12    13    10    11    31   335    35  4626    17  2302     9     2\n",
      "    42  2466    17   144   228   920     4    31]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.keras import preprocessing\n",
    "\n",
    "# Cut or complete the sentences to length = maxlen\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "\n",
    "X_train = preprocessing.sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = preprocessing.sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "print(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export train and test data\n",
    "np.save(data_path + 'X_train', X_train)\n",
    "np.save(data_path + 'y_train', y_train)\n",
    "np.save(data_path + 'X_test',  X_test)\n",
    "np.save(data_path + 'y_test',  y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export worddict\n",
    "import pickle\n",
    "\n",
    "with open(data_path + 'worddict.pickle', 'wb') as pfile:\n",
    "    pickle.dump(worddict, pfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tm]",
   "language": "python",
   "name": "conda-env-tm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
