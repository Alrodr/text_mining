{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP processing whit NLTK\n",
    "- Load an manage corpus\n",
    "- Tokenize\n",
    "- POS\n",
    "- Lemmatization and stemming\n",
    "- NER\n",
    "- Standford NLP engine\n",
    "- Pipelines fo EN and ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2.5'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "\n",
    "nltk.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/jorge/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', '/Users/jorge/anaconda3/nltk_data', '/Users/jorge/anaconda3/lib/nltk_data']\n"
     ]
    }
   ],
   "source": [
    "# The linguistic resources must be instaled in the nltk_data dir.\n",
    "\n",
    "# Check the nltk_data dir path\n",
    "print(nltk.data.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/jorge/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', '/Users/jorge/anaconda3/nltk_data', '/Users/jorge/anaconda3/lib/nltk_data', '/tmp']\n"
     ]
    }
   ],
   "source": [
    "# If you plan to use another dir, add it\n",
    "\n",
    "new_data_path='/tmp'\n",
    "nltk.data.path.append(new_data_path)\n",
    "\n",
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jorge/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download resources of NLTK data\n",
    "nltk.download('punkt') # Punkt Tokenizer Models\n",
    "\n",
    "# List of available resources here: http://www.nltk.org/nltk_data/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and manage a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/jorge/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the Brown corpus\n",
    "nltk.download('brown') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus len: 1161192\n",
      "The first 10 words: ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of']\n",
      "The first 10 tagged words: [('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN')]\n"
     ]
    }
   ],
   "source": [
    "# Then you can load it\n",
    "from nltk.corpus import brown\n",
    "\n",
    "print('Corpus len:', len(brown.words()))\n",
    "\n",
    "print('The first 10 words:', brown.words()[0:10])\n",
    "\n",
    "print('The first 10 tagged words:', brown.tagged_words()[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "- Tokenize sentences\n",
    "- Tokenize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this’s a sent tokenize test.', 'tis is sennt two.', 'is this sent three?', 'sent 4 is cool!', 'Now it’s your turn.']\n"
     ]
    }
   ],
   "source": [
    "# Use the predefined sentence tokenizer.\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"this’s a sent tokenize test. tis is sennt two. is this sent three? sent 4 is cool! Now it’s your turn.\"\n",
    "sent_tokenize_list = sent_tokenize(text)\n",
    "\n",
    "print(sent_tokenize_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this’s a sent tokenize test.',\n",
       " 'this is sent two.',\n",
       " 'is this sent three?',\n",
       " 'sent 4 is cool!',\n",
       " 'Now it’s your turn.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use explicity the punkt english sentence tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['¡Buenos días!', '¿Estas bien?']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use explicity the spanish sentence tokenizer\n",
    "spanish_tokenizer = nltk.data.load('tokenizers/punkt/spanish.pickle')\n",
    "spanish_tokenizer.tokenize('¡Buenos días! ¿Estas bien?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['greek.pickle',\n",
       " 'estonian.pickle',\n",
       " 'turkish.pickle',\n",
       " 'polish.pickle',\n",
       " 'PY3',\n",
       " 'czech.pickle',\n",
       " 'portuguese.pickle',\n",
       " 'README',\n",
       " 'dutch.pickle',\n",
       " 'norwegian.pickle',\n",
       " 'slovene.pickle',\n",
       " 'english.pickle',\n",
       " 'danish.pickle',\n",
       " 'finnish.pickle',\n",
       " 'swedish.pickle',\n",
       " 'spanish.pickle',\n",
       " 'german.pickle',\n",
       " 'italian.pickle',\n",
       " 'french.pickle']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of sentence tokenizers available in the punkt module\n",
    "os.listdir(nltk.data.path[0]+'/tokenizers/punkt/')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World', '!']\n",
      "['Ca', \"n't\", 'is', 'a', 'contraction', '.']\n"
     ]
    }
   ],
   "source": [
    "# Word tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(word_tokenize('Hello World!'))\n",
    "print(word_tokenize(\"Can't is a contraction.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Can't\", 'is', 'a', 'contraction.']\n",
      "['Can', \"'\", 't', 'is', 'a', 'contraction', '.']\n"
     ]
    }
   ],
   "source": [
    "# Others word tokenizers\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "print(tokenizer.tokenize(\"Can't is a contraction.\"))\n",
    "\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "print(tokenizer.tokenize(\"Can't is a contraction.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['El', 'Dpto', 'de', 'RR.HH', '.', 'ha', 'lanzado', '1.000', 'ofertas', 'de', 'trabajo', 'en', 'Buenos', 'Aires', '.', '3,25€', 'perdidos', 'en', 'Madrid', 'el', '2/11/2017', '.', 'Las', 'herramientas', '[', 'h1', 'y', 'h2', ']', 'son', 'compatibles', '.']\n"
     ]
    }
   ],
   "source": [
    "# word_tokenize must be used over sentences of the text\n",
    "\n",
    "text = \"El Dpto de RR.HH. ha lanzado 1.000 ofertas de trabajo en Buenos Aires. \" \\\n",
    "       \"3,25€ perdidos en Madrid el 2/11/2017. \"\\\n",
    "       \"Las herramientas [h1 y h2] son compatibles.\"\n",
    "\n",
    "sent_tok = nltk.tokenize.load('tokenizers/punkt/spanish.pickle')\n",
    "word_tok = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "\n",
    "sents = sent_tok.tokenize(text)\n",
    "\n",
    "tokens = []\n",
    "for s in sents:\n",
    "    tokens += word_tok.tokenize(s)\n",
    "    \n",
    "print(tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/jorge/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[\"Can't\", 'contraction']\n"
     ]
    }
   ],
   "source": [
    "# Stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stops = set(stopwords.words('english'))\n",
    "\n",
    "words = [\"Can't\", 'is', 'a', 'contraction']\n",
    "words_clean = [word for word in words if word not in english_stops]\n",
    "print(words_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available stopwords lists: ['arabic', 'azerbaijani', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish', 'turkish']\n",
      "Stop words spanish: ['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosostros', 'vosostras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará', 'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías', 'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas', 'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'habéis', 'han', 'haya', 'hayas', 'hayamos', 'hayáis', 'hayan', 'habré', 'habrás', 'habrá', 'habremos', 'habréis', 'habrán', 'habría', 'habrías', 'habríamos', 'habríais', 'habrían', 'había', 'habías', 'habíamos', 'habíais', 'habían', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubiéramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubiésemos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'seáis', 'sean', 'seré', 'serás', 'será', 'seremos', 'seréis', 'serán', 'sería', 'serías', 'seríamos', 'seríais', 'serían', 'era', 'eras', 'éramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fuéramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fuésemos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'tenéis', 'tienen', 'tenga', 'tengas', 'tengamos', 'tengáis', 'tengan', 'tendré', 'tendrás', 'tendrá', 'tendremos', 'tendréis', 'tendrán', 'tendría', 'tendrías', 'tendríamos', 'tendríais', 'tendrían', 'tenía', 'tenías', 'teníamos', 'teníais', 'tenían', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuviéramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened']\n"
     ]
    }
   ],
   "source": [
    "print('Available stopwords lists:', stopwords.fileids())\n",
    "\n",
    "print('Stop words spanish:', stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular expressions\n",
    "- https://www.regular-expressions.info/ \n",
    "- https://www.regextester.com/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los datos son 23.5 y -12.8 [Medidos en unidades].\n",
      "los datos son DIGIT y DIGIT  medidos en unidades .\n"
     ]
    }
   ],
   "source": [
    "# Regular expressions to clean text\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text_clean = text\n",
    "    \n",
    "    # Lowercase\n",
    "    text_clean = str.lower(text_clean)\n",
    "    \n",
    "    # Replace numbers integer, float negatives. Not replace 1 digit numbers\n",
    "    text_clean = re.sub(\"[-]?[\\d]+[.]?[\\d]+\", \"DIGIT\", text_clean)\n",
    "\n",
    "    # Delete characters [ ] { } ⋅ −\n",
    "    text_clean = re.sub('[\\[\\]/{}⋅−]+', ' ', text_clean)\n",
    "    \n",
    "    # Other cleaning options \n",
    "    \n",
    "    \n",
    "    return text_clean\n",
    "\n",
    "text = \"Los datos son 23.5 y -12.8 [Medidos en unidades].\"\n",
    "print(text)\n",
    "print(clean_text(text))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jorge/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "['dive', 'into', 'NLTK', ':', 'Part-of-speech', 'tagging', 'and', 'POS', 'Tagger']\n",
      "[('dive', 'NN'), ('into', 'IN'), ('NLTK', 'NNP'), (':', ':'), ('Part-of-speech', 'JJ'), ('tagging', 'NN'), ('and', 'CC'), ('POS', 'NNP'), ('Tagger', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "# Download the POS model\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# The POS model must be applied over word tokenized text\n",
    "text = nltk.word_tokenize(\"dive into NLTK: Part-of-speech tagging and POS Tagger\")\n",
    "print(text)\n",
    "# Use the recommended part of speech tagger\n",
    "print(nltk.pos_tag(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Understand the tagset of the POS model\n",
    "#nltk.download('tagsets')\n",
    "\n",
    "print(nltk.help.upenn_tagset('JJ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter stemmer:\n",
      "---------------\n",
      "maximum\n",
      "presum\n",
      "multipli\n",
      "provis\n",
      "say \n",
      "\n",
      "Lancaster stemmer:\n",
      "---------------\n",
      "maxim\n",
      "presum\n",
      "multiply\n",
      "provid\n",
      "say \n",
      "\n",
      "Snowball stemmer:\n",
      "---------------\n",
      "maximum\n",
      "presum\n",
      "multipli\n",
      "provis\n",
      "say\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "print('Porter stemmer:')\n",
    "print('---------------')\n",
    "print(porter_stemmer.stem('maximum'))\n",
    "print(porter_stemmer.stem('presumably'))\n",
    "print(porter_stemmer.stem('multiply'))\n",
    "print(porter_stemmer.stem('provision'))\n",
    "print(porter_stemmer.stem('saying'),'\\n')\n",
    "\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "print('Lancaster stemmer:')\n",
    "print('---------------')\n",
    "print(lancaster_stemmer.stem('maximum'))\n",
    "print(lancaster_stemmer.stem('presumably'))\n",
    "print(lancaster_stemmer.stem('multiply'))\n",
    "print(lancaster_stemmer.stem('provision'))\n",
    "print(lancaster_stemmer.stem('saying'),'\\n')\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "print('Snowball stemmer:')\n",
    "print('---------------')\n",
    "print(snowball_stemmer.stem('maximum'))\n",
    "print(snowball_stemmer.stem('presumably'))\n",
    "print(snowball_stemmer.stem('multiply'))\n",
    "print(snowball_stemmer.stem('provision'))\n",
    "print(snowball_stemmer.stem('saying'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available naguages in snowball setemmer: arabic danish dutch english finnish french german hungarian italian norwegian porter portuguese romanian russian spanish swedish\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "print('Available languages in snowball stemmer:', \" \".join(SnowballStemmer.languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'semanal'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Test for the spanish language\n",
    "from nltk.stem.snowball import SpanishStemmer\n",
    "stemmer = SpanishStemmer()\n",
    "stemmer.stem(\"Semanalmente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of lemmatizations:\n",
      "dog\n",
      "church\n",
      "abacus\n",
      "are \n",
      "\n",
      "Lemma of \"is\", no POS: is\n",
      "Lemma of \"is\", whit POS: be\n",
      "Lemma of \"are\", whit POS: be\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('wordnet') \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "print('Sample of lemmatizations:')\n",
    "print(wordnet_lemmatizer.lemmatize('dogs'))\n",
    "print(wordnet_lemmatizer.lemmatize('churches'))\n",
    "print(wordnet_lemmatizer.lemmatize('abaci'))\n",
    "print(wordnet_lemmatizer.lemmatize('are'), '\\n')\n",
    "\n",
    "# Lemmatization with POS\n",
    "\n",
    "print('Lemma of \"is\", no POS:', wordnet_lemmatizer.lemmatize('is'))\n",
    "\n",
    "print('Lemma of \"is\", whit POS:', wordnet_lemmatizer.lemmatize('is', pos='v'))\n",
    "print('Lemma of \"are\", whit POS:', wordnet_lemmatizer.lemmatize('are', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Integrating the treebank POS tags to wordnet compatible pos tags\n",
    "# - The recomended POS use different codes for the POS labels that the wordnet lemmatizer needs\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None # for easy if-statement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['You', 'are', 'good', 'friends', '.'], ['We', 'had', 'two', 'houses', 'that', 'are', 'bigger', '!']]\n",
      "[['You', 'be', 'good', 'friend', '.'], ['We', 'have', 'two', 'house', 'that', 'be', 'big', '!']]\n"
     ]
    }
   ],
   "source": [
    "text = \"You are good friends. We had two houses that are bigger!\"\n",
    "\n",
    "# Tokenize into sentences\n",
    "sents = sent_tok.tokenize(text)\n",
    "\n",
    "# for each sentence\n",
    "# - Tokenize words\n",
    "# - POS model\n",
    "# - Lemmatizer whit POS\n",
    "tokens = []\n",
    "tokens_stem = []\n",
    "for s in sents:\n",
    "    t = word_tok.tokenize(s)\n",
    "    tagged = nltk.pos_tag(t)\n",
    "    tokens += [t]\n",
    "    lemma_list = []\n",
    "    for word, tag in tagged:\n",
    "        wntag = get_wordnet_pos(tag)\n",
    "        if wntag is None: # not supply tag in case of None\n",
    "            lemma = wordnet_lemmatizer.lemmatize(word) \n",
    "        else:\n",
    "            lemma = wordnet_lemmatizer.lemmatize(word, pos=wntag) \n",
    "        lemma_list += [lemma]\n",
    "    tokens_stem += [lemma_list]\n",
    "print(tokens)\n",
    "print(tokens_stem)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
