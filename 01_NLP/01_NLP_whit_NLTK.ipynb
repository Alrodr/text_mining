{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP processing whit NLTK\n",
    "- Load an manage corpus\n",
    "- Tokenize\n",
    "- POS\n",
    "- Lemmatization and stemming\n",
    "- NER\n",
    "- Standford NLP engine\n",
    "- Pipelines fo EN and ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "\n",
    "nltk.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The linguistic resources must be instaled in the nltk_data dir.\n",
    "\n",
    "# Check the nltk_data dir path\n",
    "print(nltk.data.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you plan to use another dir, add it\n",
    "\n",
    "new_data_path='/tmp'\n",
    "nltk.data.path.append(new_data_path)\n",
    "\n",
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download resources of NLTK data\n",
    "nltk.download('punkt') # Punkt Tokenizer Models\n",
    "\n",
    "# List of available resources here: http://www.nltk.org/nltk_data/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and manage a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Brown corpus\n",
    "nltk.download('brown') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then you can import it\n",
    "from nltk.corpus import brown\n",
    "\n",
    "print('Corpus len:', len(brown.words()))\n",
    "\n",
    "print('The first 10 words:', brown.words()[0:10])\n",
    "\n",
    "print('The first 10 tagged words:', brown.tagged_words()[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "- Tokenize sentences\n",
    "- Tokenize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the predefined sentence tokenizer.\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"this’s a sent tokenize test. tis is sennt two. is this sent three? sent 4 is cool! Now it’s your turn.\"\n",
    "sent_tokenize_list = sent_tokenize(text)\n",
    "\n",
    "print(sent_tokenize_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use explicity the punkt english sentence tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use explicity the spanish sentence tokenizer\n",
    "spanish_tokenizer = nltk.data.load('tokenizers/punkt/spanish.pickle')\n",
    "spanish_tokenizer.tokenize('¡Buenos días! ¿Estas bien?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of sentence tokenizers available in the punkt module\n",
    "os.listdir(nltk.data.path[0]+'/tokenizers/punkt/')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(word_tokenize('Hello World!'))\n",
    "print(word_tokenize(\"Can't is a contraction.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Others word tokenizers\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "print(tokenizer.tokenize(\"Can't is a contraction.\"))\n",
    "\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "print(tokenizer.tokenize(\"Can't is a contraction.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_tokenize must be used over sentences of the text\n",
    "\n",
    "text = \"El Dpto de RR.HH. ha lanzado 1.000 ofertas de trabajo en Buenos Aires. \" \\\n",
    "       \"3,25€ perdidos en Madrid el 2/11/2017. \"\\\n",
    "       \"Las herramientas [h1 y h2] son compatibles.\"\n",
    "\n",
    "sent_tok = nltk.tokenize.load('tokenizers/punkt/spanish.pickle')\n",
    "word_tok = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "\n",
    "sents = sent_tok.tokenize(text)\n",
    "\n",
    "tokens = []\n",
    "for s in sents:\n",
    "    tokens += word_tok.tokenize(s)\n",
    "    \n",
    "print(tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stops = set(stopwords.words('english'))\n",
    "\n",
    "words = [\"Can't\", 'is', 'a', 'contraction']\n",
    "words_clean = [word for word in words if word not in english_stops]\n",
    "print(words_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Available stopwords lists:', stopwords.fileids())\n",
    "\n",
    "print('Stop words spanish:', stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular expressions\n",
    "- https://www.regular-expressions.info/ \n",
    "- https://www.regextester.com/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expressions to clean text\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text_clean = text\n",
    "    \n",
    "    # Lowercase\n",
    "    text_clean = str.lower(text_clean)\n",
    "    \n",
    "    # Replace numbers integer, float negatives. Not replace 1 digit numbers\n",
    "    text_clean = re.sub(\"[-]?[\\d]+[.]?[\\d]+\", \"DIGIT\", text_clean)\n",
    "\n",
    "    # Delete characters [ ] { } ⋅ −\n",
    "    text_clean = re.sub('[\\[\\]/{}⋅−]+', ' ', text_clean)\n",
    "    \n",
    "    # Other cleaning options \n",
    "    \n",
    "    \n",
    "    return text_clean\n",
    "\n",
    "text = \"Los datos son 23.5 y -12.8 [Medidos en unidades].\"\n",
    "print(text)\n",
    "print(clean_text(text))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the POS model\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# The POS model must be applied over word tokenized text\n",
    "text = nltk.word_tokenize(\"dive into NLTK: Part-of-speech tagging and POS Tagger\")\n",
    "print(text)\n",
    "# Use the recommended part of speech tagger\n",
    "print(nltk.pos_tag(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand the tagset of the POS model\n",
    "#nltk.download('tagsets')\n",
    "\n",
    "print(nltk.help.upenn_tagset('JJ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "print('Porter stemmer:')\n",
    "print('---------------')\n",
    "print(porter_stemmer.stem('maximum'))\n",
    "print(porter_stemmer.stem('presumably'))\n",
    "print(porter_stemmer.stem('multiply'))\n",
    "print(porter_stemmer.stem('provision'))\n",
    "print(porter_stemmer.stem('saying'),'\\n')\n",
    "\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "print('Lancaster stemmer:')\n",
    "print('---------------')\n",
    "print(lancaster_stemmer.stem('maximum'))\n",
    "print(lancaster_stemmer.stem('presumably'))\n",
    "print(lancaster_stemmer.stem('multiply'))\n",
    "print(lancaster_stemmer.stem('provision'))\n",
    "print(lancaster_stemmer.stem('saying'),'\\n')\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "print('Snowball stemmer:')\n",
    "print('---------------')\n",
    "print(snowball_stemmer.stem('maximum'))\n",
    "print(snowball_stemmer.stem('presumably'))\n",
    "print(snowball_stemmer.stem('multiply'))\n",
    "print(snowball_stemmer.stem('provision'))\n",
    "print(snowball_stemmer.stem('saying'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "print('Available languages in snowball stemmer:', \" \".join(SnowballStemmer.languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Test for the spanish language\n",
    "from nltk.stem.snowball import SpanishStemmer\n",
    "stemmer = SpanishStemmer()\n",
    "stemmer.stem(\"Semanalmente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet') \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "print('Sample of lemmatizations:')\n",
    "print(wordnet_lemmatizer.lemmatize('dogs'))\n",
    "print(wordnet_lemmatizer.lemmatize('churches'))\n",
    "print(wordnet_lemmatizer.lemmatize('abaci'))\n",
    "print(wordnet_lemmatizer.lemmatize('are'), '\\n')\n",
    "\n",
    "# Lemmatization with POS\n",
    "\n",
    "print('Lemma of \"is\", no POS:', wordnet_lemmatizer.lemmatize('is'))\n",
    "\n",
    "print('Lemma of \"is\", whit POS:', wordnet_lemmatizer.lemmatize('is', pos='v'))\n",
    "print('Lemma of \"are\", whit POS:', wordnet_lemmatizer.lemmatize('are', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrating the treebank POS tags to wordnet compatible pos tags\n",
    "# - The recomended POS use different codes for the POS labels that the wordnet lemmatizer needs\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None # for easy if-statement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"You are good friends. We had two houses that are bigger!\"\n",
    "\n",
    "# Tokenize into sentences\n",
    "sents = sent_tok.tokenize(text)\n",
    "\n",
    "# for each sentence\n",
    "# - Tokenize words\n",
    "# - POS model\n",
    "# - Lemmatizer whit POS\n",
    "tokens = []\n",
    "tokens_stem = []\n",
    "for s in sents:\n",
    "    t = word_tok.tokenize(s)\n",
    "    tagged = nltk.pos_tag(t)\n",
    "    tokens += [t]\n",
    "    lemma_list = []\n",
    "    for word, tag in tagged:\n",
    "        wntag = get_wordnet_pos(tag)\n",
    "        if wntag is None: # not supply tag in case of None\n",
    "            lemma = wordnet_lemmatizer.lemmatize(word) \n",
    "        else:\n",
    "            lemma = wordnet_lemmatizer.lemmatize(word, pos=wntag) \n",
    "        lemma_list += [lemma]\n",
    "    tokens_stem += [lemma_list]\n",
    "print(tokens)\n",
    "print(tokens_stem)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tm]",
   "language": "python",
   "name": "conda-env-tm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
