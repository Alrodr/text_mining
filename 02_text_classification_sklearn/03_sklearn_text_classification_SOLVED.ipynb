{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn version: 0.19.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import __version__ as sklearn_version\n",
    "print('Sklearn version:', sklearn_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data\n",
    "\n",
    "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "                 remove=('headers', 'footers', 'quotes'),\n",
    "                 categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi,\n",
      "\n",
      "\tI have a problem, I hope some of the 'gurus' can help me solve.\n",
      "\n",
      "\tBackground of the problem:\n",
      "\tI have a rectangular mesh in the uv domain, i.e  the mesh is a \n",
      "\tmapping of a 3d Bezier patch into 2d. The area in this domain\n",
      "\twhich is inside a trimming loop had to be rendered. The trimming\n",
      "\tloop is a set of 2d Bezier curve segments.\n",
      "\tFor the sake of notation: the mesh is made up of cells.\n",
      "\n",
      "\tMy problem is this :\n",
      "\tThe trimming area has to be split up into individual smaller\n",
      "\tcells bounded by the trimming curve segments. If a cell\n",
      "\tis wholly inside the area...then it is output as a whole ,\n",
      "\telse it is trivially rejected. \n",
      "\n",
      "\tDoes any body know how thiss can be done, or is there any algo. \n",
      "\tsomewhere for doing this.\n",
      "\n",
      "\tAny help would be appreciated.\n",
      "\n",
      "\tThanks, \n",
      "\tAni.\n",
      "---------------\n",
      "Target:  1\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "print(twenty_train.data[1])\n",
    "print('---------------')\n",
    "print('Target: ', twenty_train.target[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 5000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text preprocessing, tokenizing and filtering of stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=5000,\n",
    "                                stop_words='english')\n",
    "X_train_counts = tf_vectorizer.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2866)\t1\n",
      "  (0, 238)\t1\n",
      "  (0, 4522)\t1\n",
      "  (0, 2058)\t1\n",
      "  (0, 1123)\t1\n",
      "  (0, 3867)\t1\n",
      "  (0, 1543)\t1\n",
      "  (0, 3385)\t1\n",
      "  (0, 2197)\t1\n",
      "  (0, 1094)\t1\n",
      "  (0, 2643)\t1\n",
      "  (0, 1865)\t1\n",
      "  (0, 2237)\t1\n",
      "  (0, 1795)\t2\n",
      "  (0, 4520)\t1\n",
      "  (0, 2251)\t1\n",
      "  (0, 1090)\t1\n",
      "  (0, 4744)\t1\n",
      "  (0, 3276)\t1\n",
      "  (0, 357)\t1\n",
      "  (0, 3273)\t1\n",
      "  (0, 4299)\t1\n",
      "  (0, 4869)\t1\n",
      "  (0, 2014)\t1\n",
      "  (0, 2550)\t1\n",
      "  (0, 1445)\t1\n",
      "  (232, 0)\t2\n",
      "  (272, 0)\t1\n",
      "  (282, 0)\t1\n",
      "  (400, 0)\t1\n",
      "  (433, 0)\t2\n",
      "  (581, 0)\t2\n",
      "  (588, 0)\t1\n",
      "  (766, 0)\t1\n",
      "  (768, 0)\t2\n",
      "  (837, 0)\t3\n",
      "  (844, 0)\t1\n",
      "  (859, 0)\t1\n",
      "  (880, 0)\t1\n",
      "  (1030, 0)\t1\n",
      "  (1056, 0)\t6\n",
      "  (1057, 0)\t2\n",
      "  (1263, 0)\t1\n",
      "  (1475, 0)\t1\n",
      "  (1665, 0)\t16\n",
      "  (1795, 0)\t1\n",
      "  (1802, 0)\t1\n",
      "  (1833, 0)\t1\n",
      "  (1890, 0)\t2\n",
      "  (2069, 0)\t1\n",
      "  (2144, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X_train_counts[0,:])\n",
    "print(X_train_counts[:,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 5000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#From occurrences to frequencies\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer().fit(X_train_counts)\n",
    "X_train_tf = tfidf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1445)\t0.09984961017366438\n",
      "  (0, 2550)\t0.09208756192014825\n",
      "  (0, 2014)\t0.10905059471954383\n",
      "  (0, 4869)\t0.11240915977454444\n",
      "  (0, 4299)\t0.17223237883101852\n",
      "  (0, 3273)\t0.1894979846175662\n",
      "  (0, 357)\t0.19614730458851817\n",
      "  (0, 3276)\t0.23935810161052842\n",
      "  (0, 4744)\t0.2426971720743241\n",
      "  (0, 1090)\t0.18536764690499014\n",
      "  (0, 2251)\t0.2815174602044267\n",
      "  (0, 4520)\t0.23935810161052842\n",
      "  (0, 1795)\t0.32667393651341364\n",
      "  (0, 2237)\t0.2178827886891891\n",
      "  (0, 1865)\t0.1823562906613318\n",
      "  (0, 2643)\t0.09443126584369482\n",
      "  (0, 1094)\t0.25039793047338804\n",
      "  (0, 2197)\t0.22599179670408007\n",
      "  (0, 3385)\t0.27295430367067186\n",
      "  (0, 1543)\t0.16378061599510793\n",
      "  (0, 3867)\t0.16560834723130236\n",
      "  (0, 1123)\t0.1576109272623592\n",
      "  (0, 2058)\t0.14480748228447815\n",
      "  (0, 4522)\t0.12653363760378736\n",
      "  (0, 238)\t0.17006982914503366\n",
      "  (0, 2866)\t0.1903802097234869\n",
      "  (232, 0)\t0.1626733015721039\n",
      "  (272, 0)\t0.03960458829975349\n",
      "  (282, 0)\t0.08301434712372262\n",
      "  (400, 0)\t0.005277364589626779\n",
      "  (433, 0)\t0.005964993735391251\n",
      "  (581, 0)\t0.150704657006369\n",
      "  (588, 0)\t0.1542968331053262\n",
      "  (766, 0)\t0.1269568469979588\n",
      "  (768, 0)\t0.011707829878397238\n",
      "  (837, 0)\t0.334685959895095\n",
      "  (844, 0)\t0.20716739670250434\n",
      "  (859, 0)\t0.21650613403417462\n",
      "  (880, 0)\t0.01335023629162414\n",
      "  (1030, 0)\t0.27874171459307817\n",
      "  (1056, 0)\t0.21261226227802443\n",
      "  (1057, 0)\t0.13986552773816452\n",
      "  (1263, 0)\t0.08891073557106413\n",
      "  (1475, 0)\t0.2751482241616676\n",
      "  (1665, 0)\t0.2238605766396589\n",
      "  (1795, 0)\t0.09117991042239235\n",
      "  (1802, 0)\t0.016531921225093872\n",
      "  (1833, 0)\t0.11551299395013806\n",
      "  (1890, 0)\t0.007961631219200998\n",
      "  (2069, 0)\t0.10844175004957458\n",
      "  (2144, 0)\t0.11498345738389044\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tf[0,:])\n",
    "print(X_train_tf[:,0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First basic model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Define and fit in one line\n",
    "clf = MultinomialNB().fit(X_train_tf, twenty_train.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test:  0.7989347536617842\n"
     ]
    }
   ],
   "source": [
    "#Score test data\n",
    "\n",
    "# Read test data\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "                 remove=('headers', 'footers', 'quotes'),\n",
    "                 categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "# Transform text to counts\n",
    "X_test_counts = tf_vectorizer.transform(twenty_test.data)\n",
    "\n",
    "# tf-idf transformation\n",
    "X_test_tf = tfidf_transformer.transform(X_test_counts)\n",
    "\n",
    "# Prediction\n",
    "predicted = clf.predict(X_test_tf)\n",
    "\n",
    "# Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy test: ', accuracy_score(twenty_test.target, predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God loves GPU' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "# Score 2 new docs\n",
    "docs_new = ['God loves GPU', 'OpenGL on the GPU is fast']\n",
    "\n",
    "def score_text(text_list):\n",
    "    '''\n",
    "    Score function\n",
    "    '''\n",
    "    X_new_counts = tf_vectorizer.transform(docs_new)\n",
    "    X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "    predicted = clf.predict(X_new_tfidf)\n",
    "    return predicted\n",
    "\n",
    "\n",
    "for doc, category in zip(docs_new, score_text(docs_new)):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
